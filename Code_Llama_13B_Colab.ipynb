{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XEhvMyR0U_8",
        "outputId": "3a0decea-444b-4bce-e3c7-1b1c7b9af1d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zva7RUEqqLuN",
        "outputId": "c1d8f24f-b331-469c-eb64-80c0d5169bd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (921973538.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    curl -o \"https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf\"\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "curl -o \"https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7_gi0qTjxatX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'export' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!export FORCE_CMAKE=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Using cached llama_cpp_python-0.2.4.tar.gz (1.5 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.24.3)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
            "Failed to build llama-cpp-python\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [20 lines of output]\n",
            "      \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.5.0\u001b[0m using \u001b[94mCMake 3.27.4\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
            "      \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
            "      2023-09-14 16:17:34,732 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
            "      loading initial cache file C:\\Users\\intshrai\\AppData\\Local\\Temp\\tmpl41xn5hl\\build\\CMakeInit.txt\n",
            "      -- Building for: NMake Makefiles\n",
            "      CMake Error at CMakeLists.txt:3 (project):\n",
            "        Running\n",
            "      \n",
            "         'nmake' '-?'\n",
            "      \n",
            "        failed with:\n",
            "      \n",
            "         The system cannot find the file specified\n",
            "      \n",
            "      \n",
            "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
            "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
            "      -- Configuring incomplete, errors occurred!\n",
            "      \n",
            "      \u001b[91m\u001b[1m*** CMake configuration failed\u001b[0m\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for llama-cpp-python\n",
            "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-llm\n",
            "  Downloading llama_llm-0.0.15-4-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from llama-llm) (1.10.12)\n",
            "Collecting python-configuration[yaml] (from llama-llm)\n",
            "  Downloading python_configuration-0.9.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from llama-llm) (2.29.0)\n",
            "Collecting tokenizers (from llama-llm)\n",
            "  Downloading tokenizers-0.14.0-cp311-none-win_amd64.whl (2.2 MB)\n",
            "                                              0.0/2.2 MB ? eta -:--:--\n",
            "                                              0.0/2.2 MB ? eta -:--:--\n",
            "     -                                        0.1/2.2 MB 812.7 kB/s eta 0:00:03\n",
            "     -                                        0.1/2.2 MB 871.5 kB/s eta 0:00:03\n",
            "     --                                       0.1/2.2 MB 798.9 kB/s eta 0:00:03\n",
            "     ---                                      0.2/2.2 MB 817.0 kB/s eta 0:00:03\n",
            "     ---                                      0.2/2.2 MB 784.3 kB/s eta 0:00:03\n",
            "     ----                                     0.2/2.2 MB 793.0 kB/s eta 0:00:03\n",
            "     -----                                    0.3/2.2 MB 833.5 kB/s eta 0:00:03\n",
            "     ------                                   0.3/2.2 MB 846.5 kB/s eta 0:00:03\n",
            "     ------                                   0.4/2.2 MB 818.3 kB/s eta 0:00:03\n",
            "     ------                                   0.4/2.2 MB 813.8 kB/s eta 0:00:03\n",
            "     -------                                  0.4/2.2 MB 750.7 kB/s eta 0:00:03\n",
            "     --------                                 0.5/2.2 MB 782.1 kB/s eta 0:00:03\n",
            "     --------                                 0.5/2.2 MB 772.2 kB/s eta 0:00:03\n",
            "     ---------                                0.5/2.2 MB 732.5 kB/s eta 0:00:03\n",
            "     ---------                                0.5/2.2 MB 759.1 kB/s eta 0:00:03\n",
            "     ---------                                0.5/2.2 MB 740.6 kB/s eta 0:00:03\n",
            "     ----------                               0.6/2.2 MB 720.2 kB/s eta 0:00:03\n",
            "     -----------                              0.6/2.2 MB 729.7 kB/s eta 0:00:03\n",
            "     -----------                              0.6/2.2 MB 714.4 kB/s eta 0:00:03\n",
            "     -----------                              0.6/2.2 MB 712.5 kB/s eta 0:00:03\n",
            "     ------------                             0.7/2.2 MB 698.4 kB/s eta 0:00:03\n",
            "     ------------                             0.7/2.2 MB 708.0 kB/s eta 0:00:03\n",
            "     -------------                            0.7/2.2 MB 694.6 kB/s eta 0:00:03\n",
            "     -------------                            0.8/2.2 MB 693.4 kB/s eta 0:00:03\n",
            "     --------------                           0.8/2.2 MB 691.9 kB/s eta 0:00:03\n",
            "     ---------------                          0.8/2.2 MB 699.2 kB/s eta 0:00:02\n",
            "     ---------------                          0.8/2.2 MB 706.8 kB/s eta 0:00:02\n",
            "     ---------------                          0.8/2.2 MB 706.8 kB/s eta 0:00:02\n",
            "     ---------------                          0.8/2.2 MB 706.8 kB/s eta 0:00:02\n",
            "     ---------------                          0.8/2.2 MB 706.8 kB/s eta 0:00:02\n",
            "     ---------------                          0.8/2.2 MB 706.8 kB/s eta 0:00:02\n",
            "     ---------------                          0.8/2.2 MB 706.8 kB/s eta 0:00:02\n",
            "     ----------------                         0.9/2.2 MB 592.0 kB/s eta 0:00:03\n",
            "     ----------------                         0.9/2.2 MB 593.9 kB/s eta 0:00:03\n",
            "     -----------------                        1.0/2.2 MB 614.9 kB/s eta 0:00:02\n",
            "     ------------------                       1.0/2.2 MB 616.5 kB/s eta 0:00:02\n",
            "     ------------------                       1.0/2.2 MB 611.5 kB/s eta 0:00:02\n",
            "     -------------------                      1.0/2.2 MB 618.6 kB/s eta 0:00:02\n",
            "     -------------------                      1.1/2.2 MB 625.3 kB/s eta 0:00:02\n",
            "     --------------------                     1.1/2.2 MB 620.6 kB/s eta 0:00:02\n",
            "     ---------------------                    1.1/2.2 MB 621.8 kB/s eta 0:00:02\n",
            "     ---------------------                    1.2/2.2 MB 622.6 kB/s eta 0:00:02\n",
            "     ----------------------                   1.2/2.2 MB 628.3 kB/s eta 0:00:02\n",
            "     ----------------------                   1.2/2.2 MB 623.9 kB/s eta 0:00:02\n",
            "     ----------------------                   1.2/2.2 MB 619.0 kB/s eta 0:00:02\n",
            "     -----------------------                  1.3/2.2 MB 615.5 kB/s eta 0:00:02\n",
            "     -----------------------                  1.3/2.2 MB 615.5 kB/s eta 0:00:02\n",
            "     ------------------------                 1.3/2.2 MB 607.4 kB/s eta 0:00:02\n",
            "     ------------------------                 1.3/2.2 MB 608.2 kB/s eta 0:00:02\n",
            "     ------------------------                 1.4/2.2 MB 608.9 kB/s eta 0:00:02\n",
            "     -------------------------                1.4/2.2 MB 609.8 kB/s eta 0:00:02\n",
            "     -------------------------                1.4/2.2 MB 610.7 kB/s eta 0:00:02\n",
            "     ---------------------------              1.5/2.2 MB 629.1 kB/s eta 0:00:02\n",
            "     ---------------------------              1.5/2.2 MB 633.8 kB/s eta 0:00:02\n",
            "     ----------------------------             1.5/2.2 MB 633.9 kB/s eta 0:00:02\n",
            "     ----------------------------             1.6/2.2 MB 634.5 kB/s eta 0:00:01\n",
            "     -----------------------------            1.6/2.2 MB 639.0 kB/s eta 0:00:01\n",
            "     -----------------------------            1.6/2.2 MB 639.0 kB/s eta 0:00:01\n",
            "     ------------------------------           1.7/2.2 MB 628.1 kB/s eta 0:00:01\n",
            "     ------------------------------           1.7/2.2 MB 632.0 kB/s eta 0:00:01\n",
            "     -------------------------------          1.7/2.2 MB 632.4 kB/s eta 0:00:01\n",
            "     --------------------------------         1.7/2.2 MB 629.3 kB/s eta 0:00:01\n",
            "     --------------------------------         1.8/2.2 MB 636.8 kB/s eta 0:00:01\n",
            "     --------------------------------         1.8/2.2 MB 630.0 kB/s eta 0:00:01\n",
            "     ---------------------------------        1.8/2.2 MB 623.5 kB/s eta 0:00:01\n",
            "     ----------------------------------       1.9/2.2 MB 634.6 kB/s eta 0:00:01\n",
            "     -----------------------------------      2.0/2.2 MB 645.1 kB/s eta 0:00:01\n",
            "     -------------------------------------    2.0/2.2 MB 662.1 kB/s eta 0:00:01\n",
            "     -------------------------------------    2.1/2.2 MB 661.9 kB/s eta 0:00:01\n",
            "     --------------------------------------   2.1/2.2 MB 668.3 kB/s eta 0:00:01\n",
            "     ---------------------------------------  2.1/2.2 MB 665.0 kB/s eta 0:00:01\n",
            "     ---------------------------------------  2.1/2.2 MB 665.0 kB/s eta 0:00:01\n",
            "     ---------------------------------------  2.2/2.2 MB 661.6 kB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.2/2.2 MB 650.2 kB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from llama-llm) (4.65.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from llama-llm) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from llama-llm) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic->llama-llm) (4.6.3)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from python-configuration[yaml]->llama-llm) (6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->llama-llm) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->llama-llm) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->llama-llm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->llama-llm) (2023.5.7)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->llama-llm) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->llama-llm) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->llama-llm) (2.2.0)\n",
            "Collecting huggingface_hub<0.17,>=0.16.4 (from tokenizers->llama-llm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "                                              0.0/268.8 kB ? eta -:--:--\n",
            "     -----                                   41.0/268.8 kB 2.0 MB/s eta 0:00:01\n",
            "     -------------                           92.2/268.8 kB 1.8 MB/s eta 0:00:01\n",
            "     -----------------                      122.9/268.8 kB 1.2 MB/s eta 0:00:01\n",
            "     --------------------                 153.6/268.8 kB 919.0 kB/s eta 0:00:01\n",
            "     -----------------------              174.1/268.8 kB 876.1 kB/s eta 0:00:01\n",
            "     --------------------------           194.6/268.8 kB 908.0 kB/s eta 0:00:01\n",
            "     ------------------------------       225.3/268.8 kB 811.5 kB/s eta 0:00:01\n",
            "     -------------------------------      235.5/268.8 kB 761.1 kB/s eta 0:00:01\n",
            "     ------------------------------------ 268.8/268.8 kB 753.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: colorama in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->llama-llm) (0.4.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers->llama-llm) (3.9.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers->llama-llm) (2023.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\intshrai\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers->llama-llm) (23.0)\n",
            "Installing collected packages: python-configuration, huggingface_hub, tokenizers, llama-llm\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.17.1\n",
            "    Uninstalling huggingface-hub-0.17.1:\n",
            "      Successfully uninstalled huggingface-hub-0.17.1\n",
            "Successfully installed huggingface_hub-0.16.4 llama-llm-0.0.15 python-configuration-0.9.1 tokenizers-0.14.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install llama-llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4vyymsfEqx_f"
      },
      "outputs": [],
      "source": [
        "#from llama_cpp import Llama\n",
        "from llama import LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUua7nMjqa7w",
        "outputId": "8af8653b-3425-4031-d839-84f1f8c62216"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Llama' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\intshrai\\Downloads\\Code-Llama-GGUF-Demo-main\\Code-Llama-GGUF-Demo-main\\Code_Llama_13B_Colab.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/intshrai/Downloads/Code-Llama-GGUF-Demo-main/Code-Llama-GGUF-Demo-main/Code_Llama_13B_Colab.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m Llama(model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcodellama-13b-instruct.Q5_K_M.gguf\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Llama' is not defined"
          ]
        }
      ],
      "source": [
        "llm = Llama(model_path=\"codellama-13b-instruct.Q5_K_M.gguf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "l35GxqzSq7aM"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'str' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\intshrai\\Downloads\\Code-Llama-GGUF-Demo-main\\Code-Llama-GGUF-Demo-main\\Code_Llama_13B_Colab.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/intshrai/Downloads/Code-Llama-GGUF-Demo-main/Code-Llama-GGUF-Demo-main/Code_Llama_13B_Colab.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m llm(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhii\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mASSISTANT: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
          ]
        }
      ],
      "source": [
        "output = llm(\"'hii\\nASSISTANT: ' \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxt4Yk6arf2K",
        "outputId": "0f4503e2-de16-4c56-af44-2b83288c935c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'output' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\intshrai\\Downloads\\Code-Llama-GGUF-Demo-main\\Code-Llama-GGUF-Demo-main\\Code_Llama_13B_Colab.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/intshrai/Downloads/Code-Llama-GGUF-Demo-main/Code-Llama-GGUF-Demo-main/Code_Llama_13B_Colab.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output\n",
            "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Hy-TxcmIr96d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'output' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\intshrai\\Downloads\\Code-Llama-GGUF-Demo-main\\Code-Llama-GGUF-Demo-main\\Code_Llama_13B_Colab.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/intshrai/Downloads/Code-Llama-GGUF-Demo-main/Code-Llama-GGUF-Demo-main/Code_Llama_13B_Colab.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text_from_choices \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ],
      "source": [
        "text_from_choices = output['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "v-e6vVz2ssjJ",
        "outputId": "a86e1dce-ae07-4a18-8f46-cefc730d964b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Inorder and Preorder are two types of tree traversals, but they have some. The main is the order in which the of the tree are during traversal.\\n\\nInorder Traversal\\nAn inorder traversal of a binary search tree is done by the rules below:\\n- Visit the left child if it exists and recurse to further downward on the left branch.\\n- Then visit the current node.\\n- And finally, visit the right child if it exists and recurse down the right branch.\\n\\nInorder traversal is useful when we want'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_from_choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YEXvsPoutNuC"
      },
      "outputs": [],
      "source": [
        "def load_llm():\n",
        "  llm = Llama(model_path=\"codellama-13b-instruct.Q4_K_M.gguf\", n_gpu_layers=50)\n",
        "  return llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dYFY1qJxsuFk"
      },
      "outputs": [],
      "source": [
        "def llm_response(prompt):\n",
        "  llm = load_llm()\n",
        "  llm_pipeline = llm(\n",
        "      prompt,\n",
        "      max_tokens= 1096\n",
        "  )\n",
        "  text_from_choices = llm_pipeline['choices'][0]['text']\n",
        "  return text_from_choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dyqSBJ7Jt-eR"
      },
      "outputs": [],
      "source": [
        "prompt = \"'USER: What is the difference between inorder and preorder traversal? Give an example in Python.\\nASSISTANT: ' \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsMp-66guFwh",
        "outputId": "2295a8d1-4932-4e58-bd9f-7781b412b015"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "result = llm_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "WevOWsRquJbc",
        "outputId": "04e1a868-b1d3-4b91-ff17-bab88b965dcb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Inorder and Preorder are two types of tree traversals, but they have some. The main is the order in which the of the tree are during traversal.\\n\\nInorder Traversal\\nAn inorder traversal of a binary search tree is done by the rules below:\\n- Visit the left child if it exists and recurse to further downward on the left branch.\\n- Then visit the current node.\\n- And finally, visit the right child if it exists and recurse down the right branch.\\n\\nInorder traversal is useful when we want to print or process the tree in a linear fashion. For example, if we have the binary search tree below:\\n\\n       105\\n     /      \\\\ --->      /   \\\\\\n   5202     7\\n  /   \\\\    /  \\\\ /    /    \\\\\\n 2    7  12  308  15      22\\n/  \\\\        / \\\\         / \\\\\\n15  22  19  2419  24\\n\\nWe want to print the nodes of this tree in a linear fashion, we can do an inorder traversal and get the output:\\n[2, 7, 10, 15, 19, 22, 30]\\n\\nPreorder Traversal\\nA preorder traversal of a binary search tree is done by the rules below:\\n- Visit the current node and then recurse to further downward on both.\\n- First visit the left child if it exists.\\n- Then visit the right child if it exists.\\n\\nPreorder traversal is useful when we want to print or process the tree in a hierarchical fashion. For example, if we have the binary search tree below:\\n\\n       105\\n     /      \\\\ --->      /   \\\\\\n   5202     7\\n  /   \\\\    /  \\\\ /    /    \\\\\\n 2    7  12  308  15      22\\n/  \\\\        / \\\\         / \\\\\\n15'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3sWDDAxxYuK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
